{"cells":[{"cell_type":"markdown","metadata":{"id":"Hg3yVV-zjev_"},"source":["# Week2_1 Assignment\n","\n","# [BASIC](#Basic)\n","- BERT 모델의 hidden state에서 **특정 단어의 embedding을 여러 방식으로 추출 및 생성**할 수 있다.\n","\n","# [CHALLENGE](#Challenge)\n","- **cosine similarity 함수를 구현**할 수 있다. \n","- **단어들의 유사도**를 cosine similarity로 비교할 수 있다. \n","\n","# [ADVANCED](#Advanced)\n","- 문장 embedding을 구해 **문장 간 유사도**를 구할 수 있다.\n","\n","### Reference\n","- [BERT word embedding & sentence embedding tutorial 영문 블로그](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#33-creating-word-and-sentence-vectors-from-hidden-states)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"audBCE5fjSMC","executionInfo":{"status":"ok","timestamp":1646997970147,"user_tz":-540,"elapsed":7468,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}}},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import numpy as np \n","import torch\n","import random"]},{"cell_type":"markdown","metadata":{"id":"T03wL1uH1HHb"},"source":["## Basic"]},{"cell_type":"markdown","metadata":{"id":"cvJncy-fjkUV"},"source":["### BERT 모델과 토크나이저 로드   \n","- 두 사람의 대화에서 (단어 및 문장의) embedding을 생성하고자 한다. 아래 대화를 BERT 모델에 입력해 출력값 중 \"hidden states\"값을 가져오자.\n","- `Hidden States`는 3차원 텐서를 가지고 있는 list 타입이다. List에는 BERT 모델의 각 layer마다의 hidden state 3차원 텐서를 갖고 있으며 각 텐서는 (batch_size, sequence_length, hidden_size) shape을 가진다. BERT-base 모델은 12 layer를 갖고 있고 이와 별도로 Embedding Layer 1개를 더 갖고 있기 때문에 `len(hidden states)`는 13개가 된다. \n","    - batch_size: 학습 시 설정한 배치 사이즈. 또는 BERT 모델에 입력된 문장의 개수\n","    - sequence_length: 문장의 token의 개수. \n","    - hidden size: token의 embedding size \n","- Reference\n","    - [BertTokenizer.tokenize() 함수의 매개변수 설명](https://huggingface.co/transformers/v3.0.2/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__)\n","    - [BERTModel.forward() 함수의 매개변수 및 리턴 값 설명](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel.forward)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"bcmmeNkujk0x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646997980968,"user_tz":-540,"elapsed":10828,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"c9d0e450-cf73-4bea-fe5c-1a1a32e5927b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 4.6 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 43.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 40.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 32.5 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"GZqEFuRHjgBj","executionInfo":{"status":"ok","timestamp":1646997981499,"user_tz":-540,"elapsed":537,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}}},"outputs":[],"source":["from transformers import BertTokenizer, BertModel"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"NXyCD5dnjo37","colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["daa013e85ee8436fa3c099a3544c8a5d","582f441bebe7470286a18060b39f117a","8cbb38b4898347bd9b3f655f75b8e20f","dd65a8638d4c431f9d823115b99208c4","d1cb64db9a4a4f4b80c1745d95728e16","42a2614580f044daae7756b9e4ceac9d","49751e3024ee40159e6802a8cd00e74a","ab3764a8497a48269fc958628388bee4","620f3133f29e410b90c42d0cce3d64c8","2bcbb68cb7ba43cda2f2af848371286a","b1dfd7ca4ba14c349a917f4720b432d1","547224c6a8164ccb9a87d33dcbfa71fb","dd43da23e1944b95a643490796bc9e1b","56b022763c9f4d4c98cddc26d162b6ec","6a39f853a1f24d7d819ec66f20f2f222","d6250400c56a4ca2870c31847cc81075","4d693fdd87d0471cb2d512b18e5a9414","69ed666ab37145d2be214817ad6cd574","ec9b37fb79c0438aa8f8d378c51a4ac4","040f422859e24a6daff34f334e686161","72f2c45da5024c7ba85b0f43be005be0","f89fc8210a65468b89282892405b95fb","67e1fc36aadf406988f2119f5f9a0b3b","90408c0406cb432aae2c0456d3f0ab64","ea553c10af544676bc91c38e8e7556e5","d5e25466699c493185e6fb7c935813d0","8c734f34e6b24d86a95fee74df89b3d0","86a1a17a1c1e4a4cae5b7815b59c9865","ed1b1119ec9a4b758248400287427d44","1de924bd4621430baf28edf206dedc07","1a9fbc7aba8640369cf76ab3c5c73a24","f59152e785c142a6a0c26aaa9449061b","a413bda715f940779a8ff99c0265559f","a59cd219ac9e44f18603a943005da3cb","66b02518b64a4087a97fb2ebcf4174ab","357b74d7c52f49b1a4580f032d8f8701","39695bfbe22a486589088b2b712fc73b","a01556b6d349437d815a6fa5aabcad69","62933236b2304fff977c27bf357e5c77","cd3ad15cd9a8498394a279d94176ab4f","6f65cdd8245f4cc282a846d485995a3b","0e96765a9d5b4e2aa9cdd633c59a1d8a","cf68d2a6243b4938b99e0a6224e47fcc","3dd374a493fd4fc7bf0a2c3887dc8d90"]},"executionInfo":{"status":"ok","timestamp":1646998007268,"user_tz":-540,"elapsed":25782,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"bc19b404-903e-43a7-bf3e-1076949af819"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"daa013e85ee8436fa3c099a3544c8a5d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"547224c6a8164ccb9a87d33dcbfa71fb","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67e1fc36aadf406988f2119f5f9a0b3b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a59cd219ac9e44f18603a943005da3cb","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-cased\")\n","model_bert = BertModel.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Q-UbcLH4juKA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998007269,"user_tz":-540,"elapsed":21,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"c2c6bab4-a3ae-4b34-f065-0dd9f2f55160"},"outputs":[{"output_type":"stream","name":"stdout","text":["Normal Person asked: what do you do when you have free time?\n","Nerd answers: I code. code frees my minds, body and soul.\n","Normal Person asked: (what a nerd...) coding?\n","Nerd answers: Yes. coding is the best thing to do in the free time.\n"]}],"source":["normal_person = [\"what do you do when you have free time?\"]\n","nerd = [\"I code. code frees my minds, body and soul.\"]\n","normal_person.append(\"(what a nerd...) coding?\")\n","nerd.append(\"Yes. coding is the best thing to do in the free time.\")\n","\n","for i in range(len(normal_person)):\n","    print(f\"Normal Person asked: {normal_person[i]}\")\n","    print(f\"Nerd answers: {nerd[i]}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"KQtKFM-hjvVl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998007270,"user_tz":-540,"elapsed":18,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"d396d731-98a7-41f2-fcaf-bf994c5b0777"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 28])\n"]}],"source":["# 매개변수 설명\n","# truncation <- max_len 넘어가지 않도록 자르기\n","# padding <- max(seq_len, max_len) zero padding\n","# return_tensors <- return 2d tensor \n","\n","inputs = tokenizer_bert(\n","    text = normal_person,\n","    text_pair = nerd,\n","    truncation = True,\n","    padding = \"longest\", \n","    return_tensors='pt'\n","    )\n","\n","print(inputs['input_ids'].shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"zjua8EtijyJs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998011004,"user_tz":-540,"elapsed":3747,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"349d3f62-f4f4-4d21-db9b-33b934566490"},"outputs":[{"output_type":"stream","name":"stdout","text":["Coversation 0 -> '[CLS] what do you do when you have free time? [SEP] I code. code frees my minds, body and soul. [SEP] [PAD] [PAD]'\n","Coversation 1 -> '[CLS] ( what a nerd... ) coding? [SEP] Yes. coding is the best thing to do in the free time. [SEP]'\n"]}],"source":["# decoding\n","for i in range(len(inputs['input_ids'])):\n","    print(f\"Coversation {i} -> '{tokenizer_bert.decode(inputs['input_ids'][i])}'\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ggtYUkbejzBU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998011004,"user_tz":-540,"elapsed":8,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"2bffdd45-8c44-4593-c389-fdb44e6ae834"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[3463]"]},"metadata":{},"execution_count":8}],"source":["# \"code\" 단어의 token id(각 단어에게 고유하게 주어진 id)를 출력\n","tokenizer_bert.encode('code', add_special_tokens=False)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"XZUxJJxkj0NG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998011336,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"ff338c3a-351d-4fee-e4f0-cd6b0851c13d"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"S5TrsQ_Wj09O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998026582,"user_tz":-540,"elapsed":15250,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"cdd85dd8-7818-4b7c-bac7-b942aa18792d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":10}],"source":["# 입력 데이터와 BERT 모델을 \"GPU\" 장치로 로드함\n","inputs = inputs.to(device)\n","model_bert.to(device)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"_FGm_9rtj1xw","executionInfo":{"status":"ok","timestamp":1646998026583,"user_tz":-540,"elapsed":20,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}}},"outputs":[],"source":["# 입력 데이터를 BERT 모델에 넣어 출력값을 가져옴\n","outputs = model_bert(\n","    **inputs, \n","    output_hidden_states=True\n","    )"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"s9jsHuLjj3Sy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998026583,"user_tz":-540,"elapsed":19,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"c464dcd2-4887-4ad6-a494-d978fba82481"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"]},"metadata":{},"execution_count":12}],"source":["outputs.keys()"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"I0nB31XAj4M_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998026584,"user_tz":-540,"elapsed":18,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"4e850d7a-cf29-4e89-8ff1-26007f09746f"},"outputs":[{"output_type":"stream","name":"stdout","text":["# layers : 13\n","tensor shape in each layer : torch.Size([2, 28, 768])\n"]}],"source":["hidden_states = outputs['hidden_states']\n","print(f\"# layers : {len(hidden_states)}\")\n","print(f\"tensor shape in each layer : {hidden_states[-1].shape}\")"]},{"cell_type":"markdown","metadata":{"id":"mfy3I4FXj6L_"},"source":["###  Q1. 1번째 sequence (문장)에서 \"code\"라는 단어의 인덱스를 모두 반환하라.\n","- \"code\" 단어는 총 2개 존재 "]},{"cell_type":"code","execution_count":14,"metadata":{"id":"_tYG4o_ho1QY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998027247,"user_tz":-540,"elapsed":678,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"2893a168-2965-454b-e6f2-db93d0b3df1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[13, 15]\n"]}],"source":["def get_index(seq, word):\n","  index = []\n","#   word = tokenizer_bert.encode(word, add_special_tokens=False)\n","  word = tokenizer_bert.convert_tokens_to_ids(word)\n","  for i, s in enumerate(seq):\n","      if s.detach().cpu().numpy() == word:\n","          index.append(i)\n","  return index\n","\n","\n","# input\n","# seq1: 1번째 sequence\n","# token: 단어\n","seq1 = inputs['input_ids'][0]\n","token = \"code\"\n","\n","# output\n","token_index = get_index(seq1, token)\n","print(token_index)"]},{"cell_type":"markdown","source":["정답 코드"],"metadata":{"id":"q4WB8dRQuBgY"}},{"cell_type":"code","source":["# code의 index 찾기\n","word = \"code\"\n","token_id = tokenizer_bert.encode(word, add_special_tokens=False)\n","print(token_id)\n","seq1 = inputs['input_ids'][0]\n","token_index = (seq1 == token_id[0]).nonzero()#[0]\n","print(token_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxCYTwYiuCuI","executionInfo":{"status":"ok","timestamp":1646998027248,"user_tz":-540,"elapsed":38,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"ba6d1b19-895a-41a1-9aed-7377e298e62a"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[3463]\n","tensor([[13],\n","        [15]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["(seq1 == token_id[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yI19h1L9u6Tq","executionInfo":{"status":"ok","timestamp":1646998046422,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"fff56e70-af12-400c-e92f-6effc57a82bf"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([False, False, False, False, False, False, False, False, False, False,\n","        False, False, False,  True, False,  True, False, False, False, False,\n","        False, False, False, False, False, False, False, False],\n","       device='cuda:0')"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["(seq1 == token_id[0]).nonzero()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NM52R6m9u_YQ","executionInfo":{"status":"ok","timestamp":1646998062699,"user_tz":-540,"elapsed":311,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"910a63a3-200f-45a8-d500-2e954f2b8c1d"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[13],\n","        [15]], device='cuda:0')"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# validation\n","for index in token_index:\n","    assert word == tokenizer_bert.decode(seq1[index])"],"metadata":{"id":"pZIAlo0XuEwr","executionInfo":{"status":"ok","timestamp":1646998027249,"user_tz":-540,"elapsed":36,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xopkXDS2m6uA"},"source":["### Q2. 1번째 sequence의 1번째 \"code\" 토큰의 embedding을 여러가지 방식으로 구하고자 한다. BERT hidden state를 다음의 방식으로 인덱싱해 embedding을 구하라\n","- 1 layer\n","- last layer\n","- sum all 12 layers\n","- sum last 4 layers\n","- concat last 4 layers\n","- average last 4 layers"]},{"cell_type":"markdown","source":["\n","[concat last 4 layers 참고문헌](https://discuss.pytorch.kr/t/torchcat%EA%B3%BC-torchstack%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%8B%A4%EB%A5%B8%EA%B0%80%EC%9A%94/26)"],"metadata":{"id":"O0p_v9dpJcrs"}},{"cell_type":"markdown","source":["내 코드"],"metadata":{"id":"36TCBWCDvoKL"}},{"cell_type":"code","execution_count":63,"metadata":{"id":"to7IOGNwkmUS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998715429,"user_tz":-540,"elapsed":349,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"b298ad32-db45-4688-b8ea-58fbbdee054c"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 768])\n","torch.Size([1, 768])\n","torch.Size([1, 768])\n","torch.Size([1, 768])\n","torch.Size([1, 3072])\n","torch.Size([1, 768])\n"]}],"source":["# 1 layer : squeeze() 해야함\n","first_layer_emb = hidden_states[1][0][token_index[0]]\n","print(first_layer_emb.shape)\n","\n","# last layer : squeeze() 해야함\n","last_layer_emb = hidden_states[-1][0][token_index[0]]\n","print(last_layer_emb.shape)\n","\n","# sum all 12 layers : 틀림\n","sum_all_layer_emb = sum(hidden_states[1:][0])[token_index[0]]\n","print(sum_all_layer_emb.shape)\n","\n","# sum last 4 layers : 틀림\n","sum_last4_layer_emb = sum(hidden_states[-4:][0])[token_index[0]]\n","print(sum_last4_layer_emb.shape)\n","\n","# concat last 4 layers : 틀림\n","concat_last4_layer_emb = torch.concat([hidden_states[-4][0], hidden_states[-3][0], hidden_states[-2][0], hidden_states[-1][0]], dim=1)[token_index[0], :]\n","print(concat_last4_layer_emb.shape)\n","\n","# mean last 4 layers : 틀림\n","mean_last4_layer_emb = (sum(hidden_states[-4:][0])/4)[token_index[0]]\n","print(mean_last4_layer_emb.shape)"]},{"cell_type":"markdown","source":["정답 코드"],"metadata":{"id":"HqJnUrDqvm3o"}},{"cell_type":"code","source":["index = token_index[0]\n","\n","first_layer_emb = hidden_states[1][0, index.item(), :]\n","print(first_layer_emb.shape)\n","\n","last_layer_emb = hidden_states[-1][0,  index.item(), :]\n","print(last_layer_emb.shape)\n","\n","sum_all_layer_emb = sum([hs[0,  index.item(), :] for hs in hidden_states])\n","print(sum_all_layer_emb.shape)\n","\n","sum_last4_layer_emb = sum([hidden_states[i][0, index.item(), :] for i in range(len(hidden_states)-1, len(hidden_states)-1-4, -1)])\n","print(sum_last4_layer_emb.shape)\n","\n","concat_last4_layer_emb = torch.cat([hidden_states[i][0, index.item(), :] for i in range(len(hidden_states)-1, len(hidden_states)-1-4, -1)], dim=0)\n","print(concat_last4_layer_emb.shape)\n","\n","mean_last4_layer_emb = sum_last4_layer_emb / 4\n","print(mean_last4_layer_emb.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vTv58K9cvo9G","executionInfo":{"status":"ok","timestamp":1646998783121,"user_tz":-540,"elapsed":308,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"e3a0c5d9-77d1-4a89-c342-46c2553fe259"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([768])\n","torch.Size([768])\n","torch.Size([768])\n","torch.Size([768])\n","torch.Size([3072])\n","torch.Size([768])\n"]}]},{"cell_type":"markdown","metadata":{"id":"i0-C0J6o1HHl"},"source":["## Challenge"]},{"cell_type":"markdown","metadata":{"id":"H3aI-M5KubYl"},"source":["### Q3. `sum_last_four_layer` 방식으로 1번째 sequence의 2개의 \"code\" 토큰 사이의 코사인 유사도를 계산하라"]},{"cell_type":"markdown","source":["내 코드 : 틀림"],"metadata":{"id":"0oDYb4xPyZP3"}},{"cell_type":"code","execution_count":69,"metadata":{"id":"twWm2hrrp3qG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998849294,"user_tz":-540,"elapsed":304,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"1a68e1e6-885b-45e8-a98a-aeb010c10e6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n","        -1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n","        -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","        -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","        -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n","        -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n","        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n","        -1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n","        -1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000,\n","         1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","        -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","        -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","        -1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n","        -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,\n","        -1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n","        -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","        -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n","        -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n","        -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000,\n","        -1.0000, -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000,\n","        -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n","         1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n","        -1.0000,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n","         1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000],\n","       device='cuda:0', grad_fn=<DivBackward0>)\n"]}],"source":["import torch.nn.functional as F\n","\n","def cosine_similarity_manual(a, b, small_number=1e-8):\n","  # 영벡터 고려할 것\n","  result = F.cosine_similarity(x, y, dim=0, eps=small_number)\n","  return result\n","\n","# input\n","# x: 1번째 sequence의 1번째 \"code\"의 sum_last_four_layer 방식 embedding\n","# y: 1번째 sequence의 2번째 \"code\"의 sum_last_four_layer 방식 embedding\n","x = sum(hidden_states[-4:][0])[token_index[0]]\n","y = sum(hidden_states[-4:][0])[token_index[1]]\n","\n","# output\n","score = cosine_similarity_manual(x, y)\n","print(score)"]},{"cell_type":"markdown","source":["정답 코드"],"metadata":{"id":"5ysWFUehyKV-"}},{"cell_type":"code","source":["def cosine_similarity_manual(x,y,small_number=1e-8):\n","\n","    def l2_norm(a):\n","        return torch.pow(a,2).sum().sqrt()\n","    \n","    return torch.inner(x,y) / max(l2_norm(x)*l2_norm(y), small_number)"],"metadata":{"id":"n_7v9xRdyLMh","executionInfo":{"status":"ok","timestamp":1646998914802,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["sum_last4_layer_emb_1 = sum_last4_layer_emb\n","sum_last4_layer_emb_2 = sum([hidden_states[i][0, token_index[-1].item(), :] for i in range(len(hidden_states)-1, len(hidden_states)-1-4, -1)])\n","score = cosine_similarity_manual(sum_last4_layer_emb_1, sum_last4_layer_emb_2)\n","print(f\"Similariy between first 'code' and second 'code' is {score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IEna88jayP7n","executionInfo":{"status":"ok","timestamp":1646998914803,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"67d4777b-ce45-4632-9108-be834579e67e"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Similariy between first 'code' and second 'code' is 0.8426572680473328\n"]}]},{"cell_type":"markdown","metadata":{"id":"Wfc_1t2Hw8kB"},"source":["### Q4. 2번째 sequence에서 \"coding\"이라는 토큰의 위치를 반환하라"]},{"cell_type":"markdown","source":["내 코드"],"metadata":{"id":"na64V_U6ycl7"}},{"cell_type":"code","execution_count":19,"metadata":{"id":"B_OrrpEgw9pX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646998027252,"user_tz":-540,"elapsed":33,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"bd6fcf11-d0c6-4cb6-a93d-dbff47f3eff6"},"outputs":[{"output_type":"stream","name":"stdout","text":["[10, 15]\n"]}],"source":["# Q1과 동일한 문제 \n","\n","# input\n","# seq1: 2번째 sequence\n","# token: 단어\n","seq2 = inputs['input_ids'][1]\n","token = \"coding\"\n","\n","# output\n","# Q1에서 구현한 함수 사용\n","token_index2 = get_index(seq2, token)\n","print(token_index2)"]},{"cell_type":"markdown","source":["정답 코드"],"metadata":{"id":"5dhK-OF1ybnZ"}},{"cell_type":"code","source":["# code의 index 찾기 \n","word = \"coding\"\n","token_id = tokenizer_bert.encode(word, add_special_tokens=False)\n","print(token_id)\n","seq2 = inputs['input_ids'][1]\n","token_index = (seq2 == token_id[0]).nonzero()#[0]\n","print(token_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06PmQYBtycYQ","executionInfo":{"status":"ok","timestamp":1646998970510,"user_tz":-540,"elapsed":335,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"577f069a-7faa-443b-cb02-8c4a7594e265"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["[19350]\n","tensor([[10],\n","        [15]], device='cuda:0')\n"]}]},{"cell_type":"markdown","metadata":{"id":"c11W2Ht-xIwI"},"source":["### Q5. `concat_last4_layer_emb` 방식으로 2번째 sequence의 2개의 \"coding\" 토큰 사이의 코사인 유사도를 계산하라"]},{"cell_type":"markdown","source":["내 코드 : 틀림"],"metadata":{"id":"bQg6IK8FyhNe"}},{"cell_type":"code","execution_count":73,"metadata":{"id":"MXr1jtMOxKie","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646999007419,"user_tz":-540,"elapsed":326,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"39b65c43-33d9-49ae-8b19-6d6e723e660d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.5848, device='cuda:0', grad_fn=<DivBackward0>)\n"]}],"source":["# Q3과 동일한 문제\n","\n","# input\n","# x: 2번째 sequence의 1번째 \"coding\"의 concat_last4_layer_emb\n","# y: 2번째 sequence의 2번째 \"coding\"의 concat_last4_layer_emb\n","x = torch.concat([hidden_states[-4][0], hidden_states[-3][0], hidden_states[-2][0], hidden_states[-1][0]], dim=1)[token_index2[0], :]\n","y = torch.concat([hidden_states[-4][0], hidden_states[-3][0], hidden_states[-2][0], hidden_states[-1][0]], dim=1)[token_index2[1], :]\n","\n","# output\n","# Q3에서 구현한 함수 사용\n","score = cosine_similarity_manual(x, y)\n","print(score)"]},{"cell_type":"code","source":["x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DyUgZ61MynJ4","executionInfo":{"status":"ok","timestamp":1646999009532,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"bb93b27c-6bf6-4ab2-d5d9-08924a15f109"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.4401, -1.0014,  0.5484,  ...,  0.0793, -0.4005,  0.3202],\n","       device='cuda:0', grad_fn=<SliceBackward0>)"]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["정답 코드"],"metadata":{"id":"om3oHeR5yhz5"}},{"cell_type":"code","source":["concat_last4_layer_emb_1 = torch.cat([hidden_states[i][1, token_index[0].item(), :] for i in range(len(hidden_states)-1, len(hidden_states)-1-4, -1)], dim=0)\n","concat_last4_layer_emb_2 = torch.cat([hidden_states[i][1, token_index[-1].item(), :] for i in range(len(hidden_states)-1, len(hidden_states)-1-4, -1)], dim=0)\n","score =  cosine_similarity_manual(concat_last4_layer_emb_1, concat_last4_layer_emb_2)\n","print(f\"Similariy between first '{word}' and second '{word}' is {score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKptovR_yoQN","executionInfo":{"status":"ok","timestamp":1646999013745,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"bbdf7df0-c8d4-49de-d344-7423859a152f"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Similariy between first 'coding' and second 'coding' is 0.8681785464286804\n"]}]},{"cell_type":"markdown","metadata":{"id":"doBSxlvsxlZp"},"source":["### Q6. 2번째 sequence에서 랜덤하게 토큰 하나를 뽑아보자. 그 랜덤 토큰과 2번째 sequence의 2번째 \"coding\" 토큰의 코사인 유사도를 계산해보자"]},{"cell_type":"markdown","source":["내 코드 : 틀림"],"metadata":{"id":"-dcMvFR3y0Iw"}},{"cell_type":"code","execution_count":79,"metadata":{"id":"PajBEOs5xnOa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646999154746,"user_tz":-540,"elapsed":488,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"d168b09b-5442-41a6-f9ea-86e8da211cec"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.6831, device='cuda:0', grad_fn=<DivBackward0>)\n"]}],"source":["# input\n","# random_idx: random 모듈 사용하여 뽑은 랜덤 토큰의 인덱스\n","# random_word: random_idx에 해당하는 단어\n","# x: 2번째 sequence의 2번째 \"coding\" 토큰의 concat_last4_layer_emb\n","# y: 랜덤 토큰의 concat_last4_layer_emb\n","\n","random_idx = random.randrange(0, len(inputs['input_ids'][1]))\n","random_word = inputs['input_ids'][1][random_idx]\n","\n","x = torch.concat([hidden_states[-4][0], hidden_states[-3][0], hidden_states[-2][0], hidden_states[-1][0]], dim=1)[random_idx, :]\n","y = torch.concat([hidden_states[-4][0], hidden_states[-3][0], hidden_states[-2][0], hidden_states[-1][0]], dim=1)[token_index2[1], :]\n","\n","# output\n","# Q3에서 구현한 함수 사용\n","score = cosine_similarity_manual(x, y)\n","print(score)"]},{"cell_type":"code","source":["random_idx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2vjCVd4VJXn","executionInfo":{"status":"ok","timestamp":1646999155057,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"71948afa-4ea0-4baa-9b0a-bec7fe9369f1"},"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{},"execution_count":80}]},{"cell_type":"markdown","source":["정답 코드"],"metadata":{"id":"ba9gytV2y1ur"}},{"cell_type":"code","source":["token_len = hidden_states[0][0].shape[0]\n","token_len"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XbKZtp-yy2bc","executionInfo":{"status":"ok","timestamp":1646999078238,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"5b57a573-cc40-42f8-d614-863da7017a20"},"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["28"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["random_idx = random.randint(0,token_len-1)\n","print(random_idx)\n","random_token_id = inputs['input_ids'][-1][random_idx].item()\n","random_word = tokenizer_bert.decode([random_token_id])\n","print(f\"Random word : {random_word}\")\n","\n","concat_last4_layer_emb_other = torch.cat([hidden_states[i][1, random_idx, :] for i in range(len(hidden_states)-1, len(hidden_states)-1-4, -1)], dim=0)\n","score =  cosine_similarity_manual(concat_last4_layer_emb_1, concat_last4_layer_emb_other)\n","print(f\"Similariy between first '{word}' and second '{random_word}' is {score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"maYcWFo0y4rL","executionInfo":{"status":"ok","timestamp":1646999093101,"user_tz":-540,"elapsed":4,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"6b0e241c-4e93-4ed1-b055-6421e25bec13"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["6\n","Random word : .\n","Similariy between first 'coding' and second '.' is 0.5072759985923767\n"]}]},{"cell_type":"markdown","source":["#### 질문할 것!\n","```\n","inputs['input_ids'][1][random_idx]의 ids에 해당하는 단어를 가져오는 방법\n","```\n"],"metadata":{"id":"PIl0geLfvPS0"}},{"cell_type":"code","source":["random_word"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhKbkI1nVywH","executionInfo":{"status":"ok","timestamp":1646998027256,"user_tz":-540,"elapsed":27,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"bd117d4a-2ec5-48b2-f716-b9f89cc54c03"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1110, device='cuda:0')"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["tokenizer_bert.decode(random_word, add_special_tokens=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"41f0gj-rVhJe","executionInfo":{"status":"ok","timestamp":1646998027257,"user_tz":-540,"elapsed":26,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"dcc2ed2e-ac3c-4515-a52a-f326f8359437"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'i s'"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["tokenizer_bert.convert_ids_to_tokens(random_word.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"f5SJcdvpVwTn","executionInfo":{"status":"ok","timestamp":1646998027257,"user_tz":-540,"elapsed":24,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"e508896e-35c5-478a-a355-2819a872a13a"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'is'"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"hX9_cFhm1HHm"},"source":["## Advanced"]},{"cell_type":"markdown","metadata":{"id":"ub-hubXwyw9l"},"source":["### Q7. 1번째 sequence와 2번째 sequence의 문장 유사도를 구해보자. 문장의 엠베딩은 마지막 레이어의 첫번째 토큰 ('[CLS]')으로 생성한다."]},{"cell_type":"markdown","source":["내 코드 : 정답"],"metadata":{"id":"C_l8JXY9zGUn"}},{"cell_type":"code","execution_count":85,"metadata":{"id":"JpvPDM_Oyx7I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646999204627,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"7d918ec7-8e8e-4f6f-f9bc-4c1e7398b5d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.8130, device='cuda:0', grad_fn=<DivBackward0>)\n"]}],"source":["# input\n","# x: 1번째 sequence의 embedding\n","# y: 2번째 sequence의 embedding\n","x = hidden_states[-1][0][0]\n","y = hidden_states[-1][1][0]\n","\n","# output\n","# Q3에서 구현한 함수 사용\n","score =  cosine_similarity_manual(x, y)\n","print(score)"]},{"cell_type":"markdown","source":["#### 질문\n","\n","```\n","pooler_output과 last_hidden_state의 cls 토큰의 값이 다를 수 있나요?\n","```\n"],"metadata":{"id":"Aff8ukOFTm70"}},{"cell_type":"markdown","source":["정답 코드"],"metadata":{"id":"huDhD8EwzHUD"}},{"cell_type":"code","source":["sequence_1_embedding = hidden_states[-1][0, 0, :]\n","sequence_2_embedding = hidden_states[-1][1, 0, :]\n","print(f\"Shape : {sequence_1_embedding.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTd65wYkzH4N","executionInfo":{"status":"ok","timestamp":1646999193632,"user_tz":-540,"elapsed":309,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"8e68d02c-9a9f-4ed9-d7e3-eb1419a5607e"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape : torch.Size([768])\n"]}]},{"cell_type":"code","source":["score = cosine_similarity_manual(sequence_1_embedding, sequence_2_embedding).item()\n","print(f\"Similariy between the first sequence and the second sequence is {score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HG1L8cJpzUPc","executionInfo":{"status":"ok","timestamp":1646999235528,"user_tz":-540,"elapsed":4,"user":{"displayName":"Jia Son","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02994747840857838932"}},"outputId":"f3c6fe50-1e71-4ae4-85a4-683d384460df"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["Similariy between the first sequence and the second sequence is 0.8130241632461548\n"]}]}],"metadata":{"colab":{"collapsed_sections":["PIl0geLfvPS0"],"name":"Week2-1_assignment.ipynb","provenance":[]},"kernelspec":{"display_name":"torch","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"daa013e85ee8436fa3c099a3544c8a5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_582f441bebe7470286a18060b39f117a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8cbb38b4898347bd9b3f655f75b8e20f","IPY_MODEL_dd65a8638d4c431f9d823115b99208c4","IPY_MODEL_d1cb64db9a4a4f4b80c1745d95728e16"]}},"582f441bebe7470286a18060b39f117a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8cbb38b4898347bd9b3f655f75b8e20f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_42a2614580f044daae7756b9e4ceac9d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_49751e3024ee40159e6802a8cd00e74a"}},"dd65a8638d4c431f9d823115b99208c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ab3764a8497a48269fc958628388bee4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_620f3133f29e410b90c42d0cce3d64c8"}},"d1cb64db9a4a4f4b80c1745d95728e16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2bcbb68cb7ba43cda2f2af848371286a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 208k/208k [00:00&lt;00:00, 1.20MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b1dfd7ca4ba14c349a917f4720b432d1"}},"42a2614580f044daae7756b9e4ceac9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"49751e3024ee40159e6802a8cd00e74a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ab3764a8497a48269fc958628388bee4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"620f3133f29e410b90c42d0cce3d64c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2bcbb68cb7ba43cda2f2af848371286a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b1dfd7ca4ba14c349a917f4720b432d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"547224c6a8164ccb9a87d33dcbfa71fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dd43da23e1944b95a643490796bc9e1b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_56b022763c9f4d4c98cddc26d162b6ec","IPY_MODEL_6a39f853a1f24d7d819ec66f20f2f222","IPY_MODEL_d6250400c56a4ca2870c31847cc81075"]}},"dd43da23e1944b95a643490796bc9e1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"56b022763c9f4d4c98cddc26d162b6ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4d693fdd87d0471cb2d512b18e5a9414","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_69ed666ab37145d2be214817ad6cd574"}},"6a39f853a1f24d7d819ec66f20f2f222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ec9b37fb79c0438aa8f8d378c51a4ac4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_040f422859e24a6daff34f334e686161"}},"d6250400c56a4ca2870c31847cc81075":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_72f2c45da5024c7ba85b0f43be005be0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:00&lt;00:00, 502B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f89fc8210a65468b89282892405b95fb"}},"4d693fdd87d0471cb2d512b18e5a9414":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"69ed666ab37145d2be214817ad6cd574":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec9b37fb79c0438aa8f8d378c51a4ac4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"040f422859e24a6daff34f334e686161":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72f2c45da5024c7ba85b0f43be005be0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f89fc8210a65468b89282892405b95fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"67e1fc36aadf406988f2119f5f9a0b3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_90408c0406cb432aae2c0456d3f0ab64","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ea553c10af544676bc91c38e8e7556e5","IPY_MODEL_d5e25466699c493185e6fb7c935813d0","IPY_MODEL_8c734f34e6b24d86a95fee74df89b3d0"]}},"90408c0406cb432aae2c0456d3f0ab64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ea553c10af544676bc91c38e8e7556e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_86a1a17a1c1e4a4cae5b7815b59c9865","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ed1b1119ec9a4b758248400287427d44"}},"d5e25466699c493185e6fb7c935813d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1de924bd4621430baf28edf206dedc07","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1a9fbc7aba8640369cf76ab3c5c73a24"}},"8c734f34e6b24d86a95fee74df89b3d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f59152e785c142a6a0c26aaa9449061b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 8.44kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a413bda715f940779a8ff99c0265559f"}},"86a1a17a1c1e4a4cae5b7815b59c9865":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ed1b1119ec9a4b758248400287427d44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1de924bd4621430baf28edf206dedc07":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1a9fbc7aba8640369cf76ab3c5c73a24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f59152e785c142a6a0c26aaa9449061b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a413bda715f940779a8ff99c0265559f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a59cd219ac9e44f18603a943005da3cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_66b02518b64a4087a97fb2ebcf4174ab","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_357b74d7c52f49b1a4580f032d8f8701","IPY_MODEL_39695bfbe22a486589088b2b712fc73b","IPY_MODEL_a01556b6d349437d815a6fa5aabcad69"]}},"66b02518b64a4087a97fb2ebcf4174ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"357b74d7c52f49b1a4580f032d8f8701":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_62933236b2304fff977c27bf357e5c77","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cd3ad15cd9a8498394a279d94176ab4f"}},"39695bfbe22a486589088b2b712fc73b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6f65cdd8245f4cc282a846d485995a3b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":435779157,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435779157,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e96765a9d5b4e2aa9cdd633c59a1d8a"}},"a01556b6d349437d815a6fa5aabcad69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cf68d2a6243b4938b99e0a6224e47fcc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 416M/416M [00:20&lt;00:00, 19.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3dd374a493fd4fc7bf0a2c3887dc8d90"}},"62933236b2304fff977c27bf357e5c77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cd3ad15cd9a8498394a279d94176ab4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6f65cdd8245f4cc282a846d485995a3b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0e96765a9d5b4e2aa9cdd633c59a1d8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf68d2a6243b4938b99e0a6224e47fcc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3dd374a493fd4fc7bf0a2c3887dc8d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}